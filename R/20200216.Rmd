---
title: "R Notebook"
output: html_notebook
---
```{R}
# KNN
# install.packages("class")
library(class)
data(iris)
#(1)設定亂數種子
set.seed(123)
#(2)取得資料筆數
n <- nrow(iris)
#(3)取得訓練樣本數的index，70%建模，30%驗證
train_idx <- sample(seq_len(n), size = round(0.7 * n))
#(4)產出訓練資料與測試資料
traindata <- iris[train_idx,]
testdata <- iris[ - train_idx,]
train_y <- traindata[,5]
test_y <- testdata[,5]
#(5)設定K，K通常可以設定為資料筆數的平方根
k_set <- as.integer(sqrt(n))
#(6)建立模型
pred <- knn(train = traindata[-5], test = testdata[-5], cl = train_y, k = k_set)
#(7) 混淆矩陣計算準確度
message("準確度：",sum(diag(table(test_y,pred))) / sum(table(test_y,pred)) *100,"%")
```

```{r}

# 一次安裝所有packages
packages <- c("C50","tree", "rpart","randomForest")
for (i in packages){ install.packages(i) }
#一次載入packages
sapply(packages, FUN = library, character.only = TRUE)
search()

```

```{r}
#訓練樣本70%, 測試樣本30%
# install.packages("caret")
library(caret)
sample_Index <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
iris.train=iris[sample_Index,]
iris.test=iris[-sample_Index,]
#確認訓練樣本與測試樣本分不一致
par(mfrow=c(1,2))
#讓R的繪圖視窗切割成 1 X 2 的方塊

plot(iris.train$Species)
plot(iris.test$Species)
#模型訓練
iris.C50tree=C5.0(Species~ . ,data=iris.train)
summary(iris.C50tree)
plot(iris.C50tree)

#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[sample_Index]
y_hat= predict(iris.C50tree,iris.train,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=",
sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[-sample_Index]
y_hat= predict(iris.C50tree,iris.test,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=",
sum(diag(table.test))/sum(table.test)*100,"%\n")


```
```{r}
library(C50)
?C5.0
library(modeldata)
Train =  mlc_churn[1:3333,]
Test =  mlc_churn[3334:5000,]
data(mlc_churn)
str(mlc_churn)
#模型訓練
data_train = mlc_churn[,-3] # 排除 "area_code"欄位
churn.tree=rpart(churn~.,data=data_train)
churn.tree

# 繪製CART決策樹
plot(churn.tree)
text(churn.tree,cex = .8)
#cex表示字體大小

# 變數重要性
churn.tree$variable.importance

#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
y = data_train$churn
y_hat=predict(churn.tree,newdata=data_train,type="class")
table.train=table(y,y_hat)
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")

#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = Test$churn
y_hat=predict(churn.tree,newdata=Test,type="class")
table.test=table(y,y_hat)
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")


```

```{r}
#繪製 Gain Chart & Lift Chart
# setwd("c:/R_WORK")
# source('Gain_lift_Chart.r', encoding = 'Big5') #老師寫的UDF
Gain_Lift_Chart <- function (y,y_hat,y_prob) 
{
  
  #產出計算資料集 
  gain_chart_DT = cbind( y,y_hat,y_prob)
  gain_chart_DT <- as.data.frame(gain_chart_DT)
  names(gain_chart_DT) <- c("positive_flg","y_hat","yes_prob")
  
  #依照 第3個欄位 [yes_prob] 流失機率遞減排序
  gain_chart_DT = gain_chart_DT[order(gain_chart_DT[, 3], decreasing = TRUE), ]
  
  #計算累積總人數(A)
  #取index row(gain_chart_DT[, 1, drop = FALSE])
  gain_chart_DT$row_index = row(gain_chart_DT[, 1, drop = FALSE])/nrow(gain_chart_DT)
  
  #計算累積正例數
  gain_chart_DT$target_cum = cumsum(gain_chart_DT[, "positive_flg"])
  
  #計算累積正例數比例(B)
  gain_chart_DT$target_rto = gain_chart_DT$target_cum/sum(gain_chart_DT[, "positive_flg"])
  
  #計算Lift (C)
  gain_chart_DT$lift = gain_chart_DT$target_rto / gain_chart_DT$row_index
  
  return(gain_chart_DT)
} 

y = ifelse(Test$churn=='yes',1,0) #記得要把正例轉成1，負例轉成0
y_hat=predict(churn.tree,newdata=Test,type="class")
y_prob = predict(churn.tree,newdata=Test,type="prob") #預測流失機率
#呼叫老師寫的UDF
DT =Gain_Lift_Chart(y,y_hat,y_prob)
par(mfrow = c(1, 2))
# Gain Chart
plot(DT$row_index, DT$target_rto, xlab = "全體人數累積百分比", ylab = "正例人數累積百分比" ,type = "l", main = "Gain
Chart")
#Lift Chart
plot(DT$row_index, DT$lift, xlab = "全體人數累積百分比", ylab = "Lift",type = "l", main = "Lift Chart")

# 測試樣本評分
y_prob = predict(churn.tree,newdata=Test,type="prob")[,1] #取正例預測機率
# ROC Curve
# install.packages("ROCR")
library(ROCR)
pred <- prediction(y_prob, labels = Test$churn)
# tpr: True Positive Ratio 正確預測正例;
# fpr: False Positive Ration誤判為正例
perf <- performance(pred, "tpr", "fpr")
plot(perf)
points(c(0,1),c(0,1),type="l",lty=2) #畫虛線
#AUC
perf <- performance(pred, "auc")
( AUC = perf@y.values[[1]] )
( Gini = (AUC-0.5) *2 )*100


```

```{r}
library(C50)
?C5.0
library(modeldata)
Train =  mlc_churn[1:3333,]
Test =  mlc_churn[3334:5000,]
data(mlc_churn) #載入C50 churn資料集
data_train = Train[,-3] # 排除 "area_code"欄位
data_train = Train[,-1] # 排除 “state"欄位
data_train$churn = ifelse(data_train$churn=='yes',1,0) # 羅吉斯回歸預設對 y=1 建模產出推估機率

#模型訓練
logitM=glm(formula=churn~., data= data_train, family=binomial(link="logit"),na.action=na.exclude)
summary(logitM)

#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
# install.packages("InformationValue") # for optimalCutoff
library(InformationValue)
y = data_train$churn
y_hat=predict(logitM,newdata=Train,type="response")
#optimalCutoff(y, y_hat)[1] 提供了找到最佳截止值，減少錯誤分類錯誤
table.train=table(y, y_hat > optimalCutoff(y, y_hat)[1] )
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = ifelse(Test$churn=='yes',1,0)
y_hat=predict(logitM,newdata=Test,type="response")
table.test=table(y, y_hat > optimalCutoff(y, y_hat)[1] )
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
      
# ROC Curve
library(ROCR)
y_prob <- predict(logitM,newdata=Test,type="response")
y_prob <- exp(y_prob)/(1+exp(y_prob)) #odds轉機率
pred <- prediction(y_prob, labels = Test$churn)
# tpr: True Positive Ratio 正確預測正例;
# fpr: False Positive Ration誤判為正例
perf <- performance(pred, "tpr", "fpr")
plot(perf)
points(c(0,1),c(0,1),type="l",lty=2) #畫虛線
#AUC
perf <- performance(pred, "auc")
( AUC = perf@y.values[[1]] )
#Gini
( Gini = (AUC-0.5) *2 )*100        

```
```{r}
# install.packages("neuralnet") #多層神經網路:倒傳遞類神經網路
# install.packages("nnet") #單層神經網路
library(nnet)
library(neuralnet)
data(iris)
# One-hot Encoding
head(class.ind(iris$Species))
data <- cbind(iris, class.ind(iris$Species))
# 產生建模與測試樣本 7:3
n=0.3*nrow(data)
test.index=sample(1:nrow(data),n)
Train=data[-test.index,]
Test=data[test.index,]
# 建模
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
BNP = neuralnet(formula = formula.bpn,
hidden=c(5,3,2), # 兩層隱藏層，第一層有3個神經元，第二層有2個神經元
data=Train,
learningrate = 0.01, # learning rate
threshold = 0.01,
```


```{r}
stepmax = 5e5 # 最大的ieration數 = 500000(5*10^5)
)

# 繪製網路圖
plot(BNP)
# 預測
cf=compute(BNP,Test[,1:4])
Ypred = as.data.frame( round(cf$net.result) ) #預測結果
# 建立一個新欄位，叫做Species
Ypred$Species <- ""
# 把預測結果轉回Species的型態
for(i in 1:nrow(Ypred)){
if(Ypred[i, 1]==1){ Ypred[i, "Species"] <- "setosa"}
if(Ypred[i, 2]==1){ Ypred[i, "Species"] <- "versicolor"}
if(Ypred[i, 3]==1){ Ypred[i, "Species"] <- "virginica"}
}
Ypred
table(Test$Species,Ypred$Species)
# 混淆矩陣 (預測率有95.56%)
message("準確度：",sum(diag(table(Test$Species,Ypred$Species))) /
sum(table(Test$Species,Ypred$Species)) *100,"%")

```

```{r}
iris_new <- iris[, -5]
set.seed(123)
Cluster_km <- kmeans(iris_new, nstart=15,centers=3) # center就是設定群數
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 5])
#觀察分群結果與實際類別的差別
plot(iris_new $Petal.Width, iris_new $Petal.Length, col=Cluster_km$cluster)
```

```{r}
WSS_ratio <- rep(NA, times = 10)
for (k in 1:length(WSS_ratio))
{
Cluster_km <- kmeans(iris_new, nstart=15,centers=k)
WSS_ratio[k] <- Cluster_km$tot.withinss
}
#畫陡坡圖
plot(WSS_ratio, type="b", main = "陡坡圖")

```

```{r}
# install.packages("arules")
library(arules)
# (1)建置超市交易資料，7筆交易資料
transaction_data <- data.frame(Bread = c(T,T,F,T,T,F,F),
Milk = c(T,F,T,T,T,T,F),
Diaper = c(F,T,T,T,T,F,T),
Beer = c(F,T,T,T,F,F,T),
Coke = c(F,F,T,F,T,T,T)
)
# (2)建置關聯規則模型
rule=apriori(transaction_data,parameter=list(supp=0.1,conf=0.8,maxlen=4))
# (3)查看rule
inspect(rule)
summary(rule)
inspect(head(sort(rule,by="support"),20)) #依照support排序
inspect(head(sort(rule,by="confidence"),20)) #依照confidence排序
inspect(head(sort(rule,by="lift"),20)) #依照lift排序
```


```{r}
data <- read.csv("shopcart.csv",header = FALSE)
data2=as(data,"transactions") #轉呈交易檔
# 建置關聯規則模型
rule=apriori(data2,parameter=list(supp=0.2,conf=0.8,maxlen=4))
inspect(rule)
plot(rule)
plot(rule, method="graph", control=list(type="items"))
```


```{r}
```

